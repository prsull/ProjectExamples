In this project we evaluated two potential approaches for tackling the 2020 SemEval Hinglish Sentimix shared task, classifying code-mixed (i.e. Hindi and English in the same sentence) tweets writing primarily using romanized Hindi. The first approach we looked at was using a BERT. Because Multingual BERT does not cover romanized Hindi, we first compared using the pre-trained Multilingual BERT model vs a version of BERT we pretrained from scratch on Hinglish tweets. Ultimately we discovered that even though the pre-training on Multilingual BERT does not include any romanized Hindi, it performs just as well as models pre-trained purely on Hindi.

A secondary approach we investigated was using a simple Domain Adversarial Neural Network to utilize additional sentiment-labeled English language tweets, their translated and romanized counterparts, along with unlabeled codemixed hinglish tweets. The idea being that we would identify common vocabulary through the DANN that might be reflective of sentiment carrying words in both English and romanized Hindi. Unfortunately this method underperformed we believe, because of both the simplicity of the network in comparison to BERT (unable to deal with both the variational spelling, that BERT's byte pair encoding/embeddings seems to handle much better) as well as the constraint of DANNs to have 1-1 ratio of labeled to unlabled input, which meant that we could not utilize the vast quantity of Hinglish tweets that can easily be gathered on Twitter. While we identified major improvements to the improve this model, we decided a language model approach would better suit our submission for the final the task.
